[{"body":"public   BufferGetObsAsync   —   parametric type Like  MLDataPattern.BufferGetObs  but preloads observations into a buffer ring with multi - threaded workers .","id":"docstrings/DataLoaders.BufferGetObsAsync.html"},{"body":"private   batchindices   —   function Get the indices of batch  i  with batch size  size  of a collection with  n  elements . Might be a partial batch if  i  is the last batch and n  is not divisible by  size .","id":"docstrings/DataLoaders.batchindices.html"},{"body":"private   obsslices   —   function Iterate over views of all observations in a  batch . batch  can be a batched array, a tuple of batches, or a dict of batches .","id":"docstrings/DataLoaders.obsslices.html"},{"body":"But wait, there ’ s more For other dataset operations like weighted sampling, see  this section  in MLDataPattern ’ s documentation .","id":"docs/shuffling.html#but-wait-theres-more"},{"body":"Sampling and shuffling Unlike PyTorch ’ s DataLoader,  DataLoaders.jl  delegates  shuffling, subsetting and sampling  to existing packages .  Consequently there are no  shuffle ,  sampler  and  batch_sampler  arguments .","id":"docs/quickstartpytorch.html#sampling-and-shuffling"},{"body":"Automatic batching Automatic batching is controlled with the  collate  keyword argument (default  true ) .  A custom  collate_fn  is not supported .","id":"docs/quickstartpytorch.html#automatic-batching"},{"body":"public   GetObsAsync   —   parametric type","id":"docstrings/DataLoaders.GetObsAsync.html"},{"body":"Shuffling","id":"docs/shuffling.html#shuffling"},{"body":"PyTorch vs .   DataLoaders.jl","id":"docs/quickstartpytorch.html#pytorch-vs-dataloadersjl"},{"body":"Requirements droplast  works with  BatchViewBuffered works when no threads available; print error message shuffle  and  datasubset  work inplace errors on workers or main thread lead to interrupt of both compatible with  ObsDim s","id":"docs/status.html#requirements"},{"body":"At last, a realistic example Image datasets are a good use case for  DataLoaders  because 20GB (or more) of images will likely not fit into your memory, so we need an out - of - memory solution; and decoding images is CPU - bottlenecked (provided your secondary storage can keep up), so we benefit from using multiple threads . A simple data container might simply be a struct that contains the paths to a lot of image files, like so: Since we ’ re only storing the file paths and not the actual images,  ImageDataset barely takes up memory . Implementing the data container interface is straightforward: And now we can use it with  DataLoaders : To use  DataLoaders ’  multi - threading, you need to start Julia with multiple threads .  Check the number of available threads with  Threads.nthreads() .","id":"docs/datacontainers.html#at-last-a-realistic-example"},{"body":"Acknowledgements ThreadPools.jl PyTorch  DataLoader","id":"README.html#acknowledgements"},{"body":"Usage Of course, in the above example, we can keep the dataset in memory and don ’ t need parallel workers .  See  Custom data containers  for a more realistic example .","id":"README.html#usage"},{"body":"private   BatchViewCollated   —   parametric type A batch view of container  data  with collated batches of size  size .","id":"docstrings/DataLoaders.BatchViewCollated.html"},{"body":"Partial batches PyTorch ’ s  drop_last  becomes  partial  for compatibility with  Flux s  DataLoader .","id":"docs/quickstartpytorch.html#partial-batches"},{"body":"Getting Started If you get the idea and know it from PyTorch, see  Quickstart for PyTorch users . Otherwise, read on  here . Available methods are documented  here .","id":"README.html#getting-started"},{"body":"Train/validation split","id":"docs/shuffling.html#trainvalidation-split"},{"body":"Examples","id":"docs/shuffling.html#examples"},{"body":"Quickstart for PyTorch users Like Pytorch ’ s  DataLoader , this package provides an iterator over your dataset that loads data in parallel in the background . The basic interface is the same:  DataLoader(data, batchsize) . See  DataLoader  for all supported options .","id":"docs/quickstartpytorch.html#quickstart-for-pytorch-users"},{"body":"Data containers DataLoaders  supports some data containers out of the box, like arrays and tuples of arrays .  For large datasets that don ’ t fit into memory, however, we need some custom logic that loads and preprocesses our data . We can make any data container compatible with  DataLoaders  by implementing the two methods  nobs  and  getobs  from the interface package  LearnBase . nobs(ds::MyDataset)  returns the number of observations in your data container and  getobs(ds::MyDataset, idx)  loads a single observation . For performance reasons, you may want to implement  getobs!(buf, ds::MyDataset, idx) , a buffered version .","id":"docs/datacontainers.html#data-containers"},{"body":"Shuffling, subsetting, sampling Shuffling your training data every epoch and splitting a dataset into training and validation splits are common practices . While  DataLoaders  itself only provides tools to load your data effectively, using the underlying  MLDataPattern  package makes these things easy .","id":"docs/shuffling.html#shuffling-subsetting-sampling"},{"body":"Subsetting","id":"docs/shuffling.html#subsetting"},{"body":"Arguments data : A data container supporting the  LearnBase  data access pattern batchsize = 1 : Number of samples to batch together .  Disable batching by setting to  nothing","id":"docstrings/DataLoaders.DataLoader.html#arguments"},{"body":"public   DataLoader   —   function Utility for creating iterators of container  data  with a familiar interface for PyTorch users .","id":"docstrings/DataLoaders.DataLoader.html"},{"body":"public   put!   —   function Apply f !  to a buffer in  ringbuffer  and put into the results channel .","id":"docstrings/Base.put!.html"},{"body":"A simple example Let ’ s have another look at the example from the  introduction . First we create some dummy data of observations and targets: We then create a  DataLoader  with batch size 16: Et voilá, a training loop: DataLoaders  supports many different data containers by building off the interface of MLDataPattern.jl . Above, we pass in a tuple of datasets, hence the batch is also tuple . Let ’ s  next  look at a realistic use case and show how to support custom data containers .","id":"docs/simpleexample.html#a-simple-example"},{"body":"Name Module Visibility Category  GetObsAsync   DataLoaders   public   parametric type   put!   Base   public   function   BatchViewCollated   DataLoaders   private   parametric type   DataLoader   DataLoaders   public   function   BufferGetObsAsync   DataLoaders   public   parametric type   batchindices   DataLoaders   private   function   obsslices   DataLoaders   private   function ","id":"docstrings.html#docstring-index"},{"body":"TODO before v0 . 1 . 0 unify interface implement unbuffered version document format structs repr make sure datasets wrapped in  shuffleobs  and  datasubset support  getobs! provide batchview(partial = false) batchviewcollated(partial = false) AsyncIter(data, numworkers) AsyncIterBuffered(data, numworkers)","id":"docs/status.html#todo-before-v010"},{"body":"Keyword arguments partial::Bool = true : Whether to include the last batch when  nobs(dataset)  is not divisible by  batchsize .   true  ensures all batches have the same size, but some samples might be dropped parallel::Bool = Threads.nthreads() > 1) : Whether to load data in parallel, keeping the primary thread is .  Default is  true  if more than one thread is available . buffered::Bool = collate : If  buffered  is  true , loads data inplace using  getobs! .  See  Data containers  for details on buffered loading .","id":"docstrings/DataLoaders.DataLoader.html#keyword-arguments"},{"body":"ToDo fix collation for data containers that don ’ t support  getobs!","id":"docs/status.html#todo"},{"body":"DataLoaders Dev A threaded data iterator for machine learning on out - of - memory datasets .  Inspired by PyTorch ’ s DataLoader . It uses  to load data  in parallel  while keeping the primary thread free .  It can also load data  inplace  to avoid allocations . Many data containers work out of the box and it is easy to  extend with your own . DataLoaders  is built on top of and fully compatible with  MLDataPattern.jl ’ s  Data Access Pattern , a functional interface for machine learning datasets .","id":"README.html#dataloaders"},{"body":"TODO: does this make sense?","id":"src/DataLoaders.html"},{"body":"Dataset interface The dataset interface for map - style datasets is similar: PyTorch: mydataset.__getindex__(idx) mydataset.__len__() DataLoaders . jl: LearnBase.getobs(mydataset, idx) LearnBase.nobs(mydataset) See  Data containers  for specifics .","id":"docs/quickstartpytorch.html#dataset-interface-"},{"body":"Motivation Training deep learning models, data loading can quickly become a bottleneck .  When we cannot preload the dataset into memory, we have to load and preprocess it batch by batch during training . To not slow down the training, loading a batch must not take longer than one training step not block the main thread; and avoid garbage collection pauses DataLoaders uses multiple worker threads to maximize throughput keeps the main thread free for the training step; and allows buffered data loading for supporting data containers to reduce allocations","id":"docs/motivation.html#motivation"}]