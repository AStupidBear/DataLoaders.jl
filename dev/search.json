[{"body":"private   batchindices   —   function Get the indices of batch  i  with batch size  size  of a collection with  n  elements . Might be a partial batch if  i  is the last batch and n  is not divisible by  size .","id":"docstrings/DataLoaders.batchindices.html"},{"body":"private   obsslices   —   function Iterate over views of all observations in a  batch . batch  can be a batched array, a tuple of batches, or a dict of batches .","id":"docstrings/DataLoaders.obsslices.html"},{"body":"But wait, there ’ s more For other dataset operations like weighted sampling, see  this section  in MLDataPattern ’ s documentation .","id":"docs/shuffling.html#but-wait-theres-more"},{"body":"public   eachobsparallel   —   function Parallel data iterator for data container  data .  Loads data on all available threads (except the first if  useprimary  is  false ) . If  buffered  is  true , uses  getobs!  to load samples inplace . See also  MLDataPattern.eachobs eachobsparallel  does not guarantee that the samples are returned in the correct order .","id":"docstrings/DataLoaders.eachobsparallel.html"},{"body":"Sampling and shuffling Unlike PyTorch ’ s DataLoader,  DataLoaders.jl  delegates  shuffling, subsetting and sampling  to existing packages .  Consequently there are no  shuffle ,  sampler  and  batch_sampler  arguments .","id":"docs/quickstartpytorch.html#sampling-and-shuffling"},{"body":"Automatic batching Automatic batching is controlled with the  collate  keyword argument (default  true ) .  A custom  collate_fn  is not supported .","id":"docs/quickstartpytorch.html#automatic-batching"},{"body":"Shuffling","id":"docs/shuffling.html#shuffling"},{"body":"PyTorch vs .   DataLoaders.jl","id":"docs/quickstartpytorch.html#pytorch-vs-dataloadersjl"},{"body":"At last, a realistic example Image datasets are a good use case for  DataLoaders  because 20GB (or more) of images will likely not fit into your memory, so we need an out - of - memory solution; and decoding images is CPU - bottlenecked (provided your secondary storage can keep up), so we benefit from using multiple threads . A simple data container might simply be a struct that contains the paths to a lot of image files, like so: Since we ’ re only storing the file paths and not the actual images,  ImageDataset barely takes up memory . Implementing the data container interface is straightforward: And now we can use it with  DataLoaders : To use  DataLoaders ’  multi - threading, you need to start Julia with multiple threads .  Check the number of available threads with  Threads.nthreads() .","id":"docs/datacontainers.html#at-last-a-realistic-example"},{"body":"Acknowledgements MLDataPattern.jl ThreadPools.jl PyTorch  DataLoader","id":"README.html#acknowledgements"},{"body":"Usage Of course, in the above example, we can keep the dataset in memory and don ’ t need parallel workers .  See  Custom data containers  for a more realistic example .","id":"README.html#usage"},{"body":"private   BatchViewCollated   —   parametric type A batch view of container  data  with collated batches of size  size .","id":"docstrings/DataLoaders.BatchViewCollated.html"},{"body":"Overview The central function in  DataLoaders  is, of course,  DataLoader . It provides a wrapper around a parallel data iterator,  eachobsparallel Finally,  batchviewcollated  provides a lazy view of collated batches with support for inplace loading .","id":"docs/overview.html#overview"},{"body":"Partial batches PyTorch ’ s  drop_last  becomes  partial  for compatibility with  Flux s  DataLoader .","id":"docs/quickstartpytorch.html#partial-batches"},{"body":"Getting Started If you get the idea and know it from PyTorch, see  Quickstart for PyTorch users . Otherwise, read on  here . Available methods are documented  here .","id":"README.html#getting-started"},{"body":"Train/validation split","id":"docs/shuffling.html#trainvalidation-split"},{"body":"Examples","id":"docs/shuffling.html#examples"},{"body":"Quickstart for PyTorch users Like Pytorch ’ s  DataLoader , this package provides an iterator over your dataset that loads data in parallel in the background . The basic interface is the same:  DataLoader(data, batchsize) . See  DataLoader  for all supported options .","id":"docs/quickstartpytorch.html#quickstart-for-pytorch-users"},{"body":"Data containers DataLoaders  supports some data containers out of the box, like arrays and tuples of arrays .  For large datasets that don ’ t fit into memory, however, we need some custom logic that loads and preprocesses our data . We can make any data container compatible with  DataLoaders  by implementing the two methods  nobs  and  getobs  from the interface package  LearnBase . nobs(ds::MyDataset)  returns the number of observations in your data container and  getobs(ds::MyDataset, idx)  loads a single observation . For performance reasons, you may want to implement  getobs!(buf, ds::MyDataset, idx) , a buffered version .","id":"docs/datacontainers.html#data-containers"},{"body":"Shuffling, subsetting, sampling Shuffling your training data every epoch and splitting a dataset into training and validation splits are common practices . While  DataLoaders  itself only provides tools to load your data effectively, using the underlying  MLDataPattern  package makes these things easy .","id":"docs/shuffling.html#shuffling-subsetting-sampling"},{"body":"Subsetting","id":"docs/shuffling.html#subsetting"},{"body":"Arguments data : A data container supporting the  LearnBase  data access pattern batchsize = 1 : Number of samples to batch together .  Disable batching by setting to  nothing .","id":"docstrings/DataLoaders.DataLoader.html#arguments"},{"body":"public   DataLoader   —   function Utility for creating iterators of container  data  with a familiar interface for PyTorch users .","id":"docstrings/DataLoaders.DataLoader.html"},{"body":"public   put!   —   function Apply f !  to a buffer in  ringbuffer  and put into the results channel .","id":"docstrings/Base.put!.html"},{"body":"private   RingBuffer   —   parametric type A  Channel - like data structure that rotates through size  buffers .   put! s work by mutating one of the buffers: The result can then be  take! n: Only one result is valid at a time !  On the next  take! , the previous result will be reused as a buffer and be mutated by a  put! See also  put!","id":"docstrings/DataLoaders.RingBuffer.html"},{"body":"A simple example Let ’ s have another look at the example from the  introduction . First we create some dummy data of observations and targets: We then create a  DataLoader  with batch size 16: Et voilá, a training loop: DataLoaders  supports many different data containers by building off the interface of MLDataPattern.jl . Above, we pass in a tuple of datasets, hence the batch is also tuple . Let ’ s  next  look at a realistic use case and show how to support custom data containers .","id":"docs/simpleexample.html#a-simple-example"},{"body":"Documentation document data iterators better document port  PyTorch custom dataset tutorial","id":"docs/status.html#documentation"},{"body":"Name Module Visibility Category  BufferGetObsParallel   DataLoaders   private   parametric type   batchindices   DataLoaders   private   function   eachobsparallel   DataLoaders   public   function   obsslices   DataLoaders   private   function   BatchViewCollated   DataLoaders   private   parametric type   DataLoader   DataLoaders   public   function   RingBuffer   DataLoaders   private   parametric type   put!   Base   public   function ","id":"docstrings.html#docstring-index"},{"body":"Examples DataLoader(data, 16) ===  BufferGetObsParallel ( batchviewcollated (data, 16))","id":"docstrings/DataLoaders.DataLoader.html#examples"},{"body":"Keyword arguments partial::Bool = true : Whether to include the last batch when  nobs(dataset)  is not divisible by  batchsize .   true  ensures all batches have the same size, but some samples might be dropped . buffered::Bool = collate : If  buffered  is  true , loads data inplace using  getobs! .  See  Data containers  for details on buffered loading . parallel::Bool = Threads.nthreads() > 1) : Whether to load data in parallel, keeping the primary thread is .  Default is  true  if more than one thread is available . useprimary::Bool = false : If  false , keep the main thread free when loading data in parallel .  Is ignored if  parallel  is  false .","id":"docstrings/DataLoaders.DataLoader.html#keyword-arguments"},{"body":"DataLoaders Documentation (latest) A threaded data iterator for machine learning on out - of - memory datasets .  Inspired by PyTorch ’ s DataLoader . It uses  to load data  in parallel  while keeping the primary thread free .  It can also load data  inplace  to avoid allocations . Many data containers work out of the box and it is easy to  extend with your own . DataLoaders  is built on top of and fully compatible with  MLDataPattern.jl ’ s  Data Access Pattern , a functional interface for machine learning datasets .","id":"README.html#dataloaders"},{"body":"Dataset interface The dataset interface for map - style datasets is similar: PyTorch: mydataset.__getindex__(idx) mydataset.__len__() DataLoaders . jl: LearnBase.getobs(mydataset, idx) LearnBase.nobs(mydataset) See  Data containers  for specifics .","id":"docs/quickstartpytorch.html#dataset-interface-"},{"body":"Features make compatible with  ObsDim s","id":"docs/status.html#features"},{"body":"TODOs","id":"docs/status.html#todos"},{"body":"private   BufferGetObsParallel   —   parametric type Like  MLDataPattern.BufferGetObs  but preloads observations into a buffer ring with multi - threaded workers .","id":"docstrings/DataLoaders.BufferGetObsParallel.html"},{"body":"Motivation Training deep learning models, data loading can quickly become a bottleneck .  When we cannot preload the dataset into memory, we have to load and preprocess it batch by batch during training . To not slow down the training, loading a batch must not take longer than one training step not block the main thread; and avoid garbage collection pauses DataLoaders uses multiple worker threads to maximize throughput keeps the main thread free for the training step; and allows buffered data loading for supporting data containers to reduce allocations","id":"docs/motivation.html#motivation"}]